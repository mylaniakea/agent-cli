{
  "ollama": {
    "llama2": {
      "context_length": 4096,
      "max_tokens": 2048,
      "supports_streaming": true,
      "supports_history": true
    },
    "mistral": {
      "context_length": 8192,
      "max_tokens": 4096,
      "supports_streaming": true,
      "supports_history": true
    },
    "llama3": {
      "context_length": 8192,
      "max_tokens": 4096,
      "supports_streaming": true,
      "supports_history": true
    },
    "codellama": {
      "context_length": 16384,
      "max_tokens": 8192,
      "supports_streaming": true,
      "supports_history": true
    }
  },
  "openai": {
    "gpt-4": {
      "context_length": 8192,
      "max_tokens": 4096,
      "supports_streaming": true,
      "supports_history": true,
      "default_temperature": 0.7
    },
    "gpt-4-turbo": {
      "context_length": 128000,
      "max_tokens": 4096,
      "supports_streaming": true,
      "supports_history": true,
      "default_temperature": 0.7
    },
    "gpt-3.5-turbo": {
      "context_length": 16385,
      "max_tokens": 4096,
      "supports_streaming": true,
      "supports_history": true,
      "default_temperature": 0.7
    },
    "gpt-3.5-turbo-16k": {
      "context_length": 16385,
      "max_tokens": 4096,
      "supports_streaming": true,
      "supports_history": true,
      "default_temperature": 0.7
    }
  },
  "anthropic": {
    "claude-3-opus-20240229": {
      "context_length": 200000,
      "max_tokens": 4096,
      "supports_streaming": true,
      "supports_history": true,
      "default_temperature": 1.0
    },
    "claude-3-sonnet-20240229": {
      "context_length": 200000,
      "max_tokens": 4096,
      "supports_streaming": true,
      "supports_history": true,
      "default_temperature": 1.0
    },
    "claude-3-haiku-20240307": {
      "context_length": 200000,
      "max_tokens": 4096,
      "supports_streaming": true,
      "supports_history": true,
      "default_temperature": 1.0
    }
  },
  "google": {
    "gemini-pro": {
      "context_length": 32768,
      "max_tokens": 2048,
      "supports_streaming": true,
      "supports_history": true,
      "default_temperature": 0.9
    },
    "gemini-1.5-pro": {
      "context_length": 1000000,
      "max_tokens": 8192,
      "supports_streaming": true,
      "supports_history": true,
      "default_temperature": 0.9
    },
    "gemini-1.5-flash": {
      "context_length": 1000000,
      "max_tokens": 8192,
      "supports_streaming": true,
      "supports_history": true,
      "default_temperature": 0.9
    }
  }
}

